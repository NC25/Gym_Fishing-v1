{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spinning up AI TD3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6MsumpXvvrgwvnb8Z3/oq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NC25/Gym_Fishing-v1/blob/master/fishing-v1/spinning_up_td3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUyG5YNwtpRF"
      },
      "source": [
        "Steps for Custom algortihims\n",
        "\n",
        "1.   Logger (probility distribution)\n",
        "\n",
        "2.   Set seed\n",
        "\n",
        "1.   Instantiate env\n",
        "\n",
        "1.   Placeholders for computation\n",
        "\n",
        "1.   Experience Buffer\n",
        "2.  Helper Functions\n",
        "\n",
        "7.   Running main loop\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjypK4wHtdGg"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o9Rag7Dthx8"
      },
      "source": [
        "!git clone https://github.com/boettiger-lab/gym_fishing.git\n",
        "!python gym_fishing/setup.py sdist bdist_wheel\n",
        "!pip install -e ./gym_fishing/\n",
        "!ls\n",
        "!cd gym_fishing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaHUEO_Xth0-"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('INFO')\n",
        "tf.autograph.set_verbosity(0)\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OsyRm7HukxW"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines.common.policies import LstmPolicy\n",
        "from stable_baselines import TD3\n",
        "from stable_baselines.td3.policies import MlpPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.ddpg.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TRz4SW6uk0q"
      },
      "source": [
        "import gym_fishing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXVQNZCz5GAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3451e0d1-3033-486d-810f-5c97597982ff"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e3-5yfZvpFo"
      },
      "source": [
        "# Stable Baselines: Custom Actor-Critic Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x27rnj0qMiqS"
      },
      "source": [
        "from stable_baselines.common.policies import ActorCriticPolicy, register_policy, nature_cnn\n",
        "from stable_baselines.common.distributions import DiagGaussianProbabilityDistribution\n",
        "\n",
        "\"\"\"\n",
        "    Policy object that implements actor critic\n",
        "\n",
        "    :sess: Current TensorFlow session\n",
        "    :param ob_space: The observation space of env\n",
        "    :param ac_space: (Gym Space) The action space of the environment\n",
        "    :param n_env: (int) The number of environments to run for multiprocessing\n",
        "    :param n_steps: (int) The number of steps to run for each environment\n",
        "    :param n_batch: (int) The number of batchs to run (n_envs * n_steps)\n",
        "    :param reuse: (bool) If the policy is reusable or not\n",
        "    :param scale: (bool) whether or not to scale the input\n",
        "    \"\"\"\n",
        "\n",
        "#Custom MLP Policy: Actor has 3 layers of size 128, Critic has 2 layers of size 32\n",
        "# with a nature_cnn feature extractor\n",
        "\n",
        "class CustomPolicy(ActorCriticPolicy):\n",
        "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **kwargs):\n",
        "        super(CustomPolicy, self).__init__(sess, ob_space, ac_space, n_env=n_env, n_steps=n_steps, n_batch=n_batch, reuse=reuse, scale=True)\n",
        "\n",
        "        self._probability_distribution = DiagGaussianProbabilityDistribution\n",
        "        self._pdtype = make_proba_dist_type(ac_space)\n",
        "        self._policy = None\n",
        "        self._proba_distribution = None\n",
        "        self._value_fn = None\n",
        "        self._action = None\n",
        "        self._deterministic_action = None\n",
        "        self.n_env = 2\n",
        "        self.n_steps = 1000\n",
        "        n_batch = 2000\n",
        "\n",
        "        with tf.variable_scope(\"model\", reuse=reuse):\n",
        "            activation = tf.nn.relu\n",
        "\n",
        "            output = nature_cnn(self.processed_obs, **kwargs)\n",
        "            output = tf.layers.flatten(output)\n",
        "\n",
        "            pi_h = output\n",
        "            for i, layer_size in enumerate([128, 128, 128]):\n",
        "                pi_h = activ(tf.layers.dense(pi_h, layer_size, name='pi_fc' + str(i)))\n",
        "            pi_latent = pi_h\n",
        "\n",
        "            vf_h = output\n",
        "            for i, layer_size in enumerate([32, 32]):\n",
        "                vf_h = activ(tf.layers.dense(vf_h, layer_size, name='vf_fc' + str(i)))\n",
        "            value_fn = tf.layers.dense(vf_h, 1, name='vf')\n",
        "            vf_latent = vf_h\n",
        "\n",
        "            self._proba_distribution, self._policy, self.q_value = \\\n",
        "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
        "\n",
        "        self._value_fn = value_fn\n",
        "        self._setup_init()\n",
        "\n",
        "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
        "        if deterministic:\n",
        "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
        "                                                   {self.obs_ph: obs})\n",
        "        else:\n",
        "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
        "                                                   {self.obs_ph: obs})\n",
        "        return action, value, self.initial_state, neglogp\n",
        "\n",
        "    def proba_step(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
        "\n",
        "    def value(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self.value_flat, {self.obs_ph: obs})\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn949xvJSIHJ"
      },
      "source": [
        "env = DummyVecEnv([lambda: gym.make('fishing-v1')])\n",
        "\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "model = TD3(CustomPolicy, env, action_noise=action_noise, verbose=1)\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG-CZng8SsI-"
      },
      "source": [
        "# Spinning AI: Custom Neural Network and Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxTMsK5Z7zuU"
      },
      "source": [
        "def rnn():\n",
        "  inputs = keras.Input(shape=(env.observation_space.shape))\n",
        "  tf.keras.utils.normalize(inputs, axis=-1, order=2)\n",
        "  layer_1 = layers.Dense(64, activation=\"relu\")\n",
        "  x = layer_1(inputs)\n",
        "  layer_2 = layers.Dense(32, activation=\"relu\")(x)\n",
        "  x = layer_2(inputs)\n",
        "  output = layers.Dense(8, activation='relu') \n",
        "  x = output(inputs)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs, name=None)\n",
        "  inputs.reshape(3, 2)\n",
        "  model.add(layers.LSTM(64))\n",
        "  model.add(layers.Dense(8))\n",
        "  output = model.compile(loss=mean_squared_error, optimizer='adam')\n",
        "  return output\n",
        "\n",
        "rnn\n",
        "#This will already be done when we make the policy or call agent\n",
        "#model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pojVz710u2zx"
      },
      "source": [
        "def get_vars(scope):\n",
        "  return[x for x in tf.global_variables() if scope in x.name]\n",
        "\n",
        "def count_vars(scope):\n",
        "  v = get_vars(scope)\n",
        "  return sum(np.prod(var.shape.as_list()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdDzOCiy17FZ"
      },
      "source": [
        "!git clone https://github.com/openai/spinningup.git\n",
        "!cd spinningup\n",
        "!pip install -e ./spinningup/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2d-HzDl0-Lt"
      },
      "source": [
        "import spinningup\n",
        "from logx import EpochLogger\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jfprpc_3Ddz"
      },
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, obs_dim, act_dim, size):\n",
        "\n",
        "    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n",
        "    self.rewards_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.output_buf = np.zeros(sizem dtype=np.float32)\n",
        "    self.ptr, self.size, self.max_size= 0,0, size\n",
        "\n",
        "    def store(self, obs, act, rew, next_obs, done):\n",
        "        self.obs1_buf[self.ptr] = obs\n",
        "        self.obs2_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rewards_buf[self.ptr] = rew\n",
        "        self.outputs_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "        return dict(obs1=self.obs1_buf[idxs],\n",
        "                    obs2=self.obs2_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rewards_buf[idxs],\n",
        "                    done=self.output_buf[idxs])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmoFR_RF550G"
      },
      "source": [
        "import core"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KbmhDrO5oX5"
      },
      "source": [
        "def td3(env_fn, actor_critic=lstm_actor_critic(), ac_kwargs=dict(), seed=0, \n",
        "        steps_per_epoch=1000, epochs=300, replay_size=int(1e3), gamma=0.99, \n",
        "        polyak=0.995, pi_lr=1e-3, q_lr=1e-3, batch_size=100, start_steps=10000, \n",
        "        update_after=1000, update_every=50, act_noise=0.1, target_noise=0.2, \n",
        "        noise_clip=0.5, policy_delay=2, num_test_episodes=10, max_ep_len=1000, \n",
        "        logger_kwargs=dict(), save_freq=1):\n",
        "  \n",
        "  #polyak: averaging for target networks\n",
        "  #pi_lr: learning rate for policy\n",
        "  #q_lr: learneing rate for Q-networks\n",
        "  #update_every: updates after every number of steps\n",
        "\n",
        "  logger = EpochLogger(**logger_kwargs)\n",
        "  logger.save_config(locals())\n",
        "\n",
        "  tf.set_random_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "#set dimensions\n",
        "  env = env_fn()\n",
        "  observation_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.shape[0]\n",
        "\n",
        "  ac_kwargs['action_space'] = env.action_space\n",
        "  #make all the dimensions the same bound\n",
        "\n",
        "\n",
        "  #Target policy networks\n",
        "  with tf.variable_scope('target'):\n",
        "        pi_targ, _, _, _  = actor_critic(x2_ph, a_ph, **ac_kwargs)\n",
        "    \n",
        "  # Target Q networks\n",
        "  with tf.variable_scope('target', reuse=True):\n",
        "\n",
        "    #add noise\n",
        "    epsilon = tf.random_normal(tf.shape(pi_targ), stddev=target_noise)\n",
        "    epsilon = tf.clip_by_value(epsilon, -noise_clip, noise_clip)\n",
        "    smoothed = pi_targ + epsilon\n",
        "    smoothed = tf.clip_by_value(smoothed, -act_limit, act_limit)\n",
        "\n",
        "    #set target Q values\n",
        "    _, q1_targ, q2_targ, _ = actor_critic(x2_ph, a2, **ac_kwargs)\n",
        "\n",
        "  #Experience Buffer\n",
        "  replay_buffer = ReplayBuffer(observation_dim=observation_dim, action_dim=action_dim, size=replay_size)\n",
        "\n",
        "  #Add Bellman Backup for stability\n",
        "  min_q_target = tf.minimum(q1_target, q2_target)\n",
        "  \n",
        "  backup = tf.stop_gradient(r_ph + gamma*(1-d_p)*min_q_target)\n",
        "\n",
        "  #Loss function for TD3\n",
        "  pi_loss = -tf.reduce_mean(q1_pi)\n",
        "  q1_loss = tf.reduce_mean((q1-backup)**2)\n",
        "  q2_loss = tf.reduce.mean((q2-backup)**2)\n",
        "  q_loss = q1_loss + q2_loss\n",
        "\n",
        "\n",
        "  #optimizer\n",
        "  pi_optimizer = tf.train.AdamOptimizer(learning_rate=pi_lr)\n",
        "  q_optimizer = tf.train.AdamOptimizer(learning_rate=q_lr)\n",
        "  train_pi_op = pi_optimizer.minimize(pi_loss, var_list=get_vars('main/pi'))\n",
        "  train_q_op = q_optimizer.minimize(q_loss, var_list=get_vars('main/q'))\n",
        "\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(target_init)\n",
        "\n",
        "  #save weights\n",
        "  logger.setup_tf_saver(sess, inputs={'x': x_ph, 'a': a_ph}, outputs={'pi': pi, 'q1': q1, 'q2': q2})\n",
        "\n",
        "  def step(observation, noise_scale):\n",
        "  action = sess.run(pi, feed_dict={x_ph: observation.reshape(1, -1)})\n",
        "  action += noise_scale + np.random.randn(action_dim)\n",
        "  return np.clip(a, -act_limit, act_limit)\n",
        "\n",
        "  def sample():\n",
        "  for i in range(num_test_episodes):\n",
        "    observation = env.reset()\n",
        "    while ep_len <= max_ep_len:\n",
        "      observation, reward, done, _ = env.step(action(observation, 0))\n",
        "      ep_return += reward\n",
        "      ep_len += 1\n",
        "      d = False if ep_len==max_ep_len else d\n",
        "      replay_buffer.store(observation, action, reward, observation_2, done)\n",
        "      observation = observation_2\n",
        "    logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfjIvYgUvFtn"
      },
      "source": [
        "from spinup.utils.run_utils import setup_logger_kwargs\n",
        "logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed)\n",
        "\n",
        "\n",
        "td3()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}