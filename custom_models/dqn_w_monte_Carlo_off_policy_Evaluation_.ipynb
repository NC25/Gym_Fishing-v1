{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN w/ Monte Carlo Off-Policy Evaluation .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NC25/Gym_Fishing-v1/blob/master/custom_models/dqn_w_monte_Carlo_off_policy_Evaluation_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ZT5XvSp2jD"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj8H7uHXp75a"
      },
      "source": [
        "!git clone https://github.com/boettiger-lab/gym_fishing.git\n",
        "!python gym_fishing/setup.py sdist bdist_wheel\n",
        "!pip install -e ./gym_fishing/\n",
        "!ls\n",
        "!cd gym_fishing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No5hk3f2p-fR"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('INFO')\n",
        "tf.autograph.set_verbosity(0)\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlv785oXqCIx"
      },
      "source": [
        "import gym_fishing\n",
        "import pandas as pd\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import PPO2\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines import DQN\n",
        "from stable_baselines.deepq.policies import MlpPolicy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdX8tb1pqENR"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIkkH39SqHhc"
      },
      "source": [
        "env = gym.make('fishing-v0')\n",
        "model = DQN(MlpPolicy, env , verbose=2)\n",
        "trained_model = model.learn(total_timesteps=10000)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxois0ujqvgf"
      },
      "source": [
        "from pylab import rcParams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTN5ZYGrq7UU"
      },
      "source": [
        "# Add Monte Carlo Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wji3tr3es4wz"
      },
      "source": [
        "def mc_control_epsilon_greedy(env, num_episodes, discount=.99):\n",
        "\n",
        "  \"\"\"\n",
        "  Monte Carlo Prediction - Estimates value function through sampling\n",
        "  \"\"\"\n",
        "\n",
        "  #set variables as dictionaries with type float\n",
        "  \n",
        "  return_sum = defaultdict(float)\n",
        "  #return_occurence = defaultdict(float)\n",
        "\n",
        "  Value = defaultdict(float)\n",
        "\n",
        "  for i in range(1, num_episodes + 1):\n",
        "    if i % 1 ==0:\n",
        "      print(\"\\rEpisode {}/{}\".format(i, num_episodes), end=\"\") #carriage return\n",
        "            \n",
        "\n",
        "#Generate episode with (state, action, reward) tuple\n",
        "\n",
        "    episode = []\n",
        "    obs = env.reset()\n",
        "    for i in range(num_episodes):\n",
        "      action, _states = trained_model.predict(obs)\n",
        "      next_obs, reward, done, info = env.step(action)\n",
        "      episode.append((obs, action, reward))\n",
        "      if done:\n",
        "        break\n",
        "      obs = next_obs \n",
        "    \n",
        "    #convert to tuple\n",
        "    tuple(episode)\n",
        "    for x in episode:\n",
        "      all_episode_obs = set([tuple(x[0])])\n",
        "      for i in range(obs):\n",
        "        if x[0] == obs:\n",
        "          first_occurence = x in enumerate(episode)\n",
        "\n",
        "     # first_occurence = next(i for i, x in enumerate(episode) if x[0] == obs)\n",
        "      #obtain cumulative return\n",
        "      reward_sum = sum([x[2] * (discount**i) for i, x in enumerate(episode[first_occurence:])])\n",
        "\n",
        "      return_sum[tuple(obs)] += reward_sum\n",
        "      return_occurence = 0\n",
        "      while x <= num_episodes:\n",
        "        return_occurence += 1\n",
        "        if return_occurence == num_episodes:\n",
        "          final\n",
        "\n",
        "      return_occurence += 1.0\n",
        "\n",
        "      Value[tuple(obs)] = return_sum[tuple(obs)] / return_occurence\n",
        "    \n",
        "  return Value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roojNSNH4U4E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "8654ab9a-2120-479b-9f7d-b538a3a513d9"
      },
      "source": [
        "prediction = estimate(env, num_episodes=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpisode 1/100"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-57b6a8e18fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-4c00927d4e8d>\u001b[0m in \u001b[0;36mestimate\u001b[0;34m(env, num_episodes, discount)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mall_episode_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0mfirst_occurence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmbA7ICbD5rd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "e3b9a46c-e2db-4d6f-9a92-3e1d64457bdb"
      },
      "source": [
        "\n",
        "def weighted_sampling(env, num_episodes, discount = .99):\n",
        "    \"\"\"\n",
        "    Monte Carlo Control Off-Policy Control using Weights for Sampling.\n",
        "    Finds an optimal greedy policy.\n",
        "    \"\"\"\n",
        "    \n",
        "    # creates Q dictionary that maps obs to action values\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space))\n",
        "    #dictionary for weights\n",
        "    C = defaultdict(lambda: np.zeros(env.action_space))\n",
        "    \n",
        "    # learn greey policy\n",
        "    target_policy = env.step(Q)\n",
        "        \n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        if i_episode % 1 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
        "\n",
        "        # Generate an episode to be tuple (state, action, reward) tuples\n",
        "        episode = []\n",
        "        obs = env.reset()\n",
        "        for t in range(100):\n",
        "            # Sample an action from our policy\n",
        "            action, _states = trained_model.predict(obs)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            if done:\n",
        "                break\n",
        "            obs = next_obs\n",
        "        \n",
        "        # Sum of discounted returns\n",
        "        G = 0.0\n",
        "        # weights for return\n",
        "        W = 1.0\n",
        "        for t in range(len(episode))[::-1]:\n",
        "            obs, action, reward = episode[t]\n",
        "            G = discount * G + reward\n",
        "            #  Add weights\n",
        "            C[obs][action] += W\n",
        "            # Update policy\n",
        "            Q[obs][action] += (W / C[obs][action]) * (G - Q[obs][action]\n",
        "                                                      \n",
        "            if action !=  np.argmax(target_policy(obs)):\n",
        "                break\n",
        "            W = W * 1./behavior_policy(obs)[action]\n",
        "        \n",
        "    return Q, target_policy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-29b5ec7b58b7>\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    if action !=  np.argmax(target_policy(obs)):\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UROkH6sKEhN_"
      },
      "source": [
        "Q, policy = mc_control_importance_sampling(env, num_episodes=500000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}